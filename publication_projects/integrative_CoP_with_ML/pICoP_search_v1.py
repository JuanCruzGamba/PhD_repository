# Import libraries
import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import StratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from tqdm import tqdm 

# The evaluator_kfold function, for performing cross-validation
def evaluator_kfold(X, y, feature_name, num_seeds=100, num_folds=5):
    """
    Evaluates a single feature using repeated stratified k-fold cross-validation
    with different random seeds. It calculates precision, recall, f1-score for
    both class 0 and class 1, overall accuracy, and AUC-ROC on the test sets.
    
    Parameters:
        X: pd.Series or np.array containing the feature values.
        y: pd.Series or np.array containing the target labels.
        feature_name: str, the name of the feature being evaluated, used for reporting.
        num_seeds: int, the number of repetitions with different random seeds for the
                   StratifiedKFold splitter.
        num_folds: int, the number of folds for the stratified cross-validation.
    
    Returns:
        tuple:
            - feature_name (str): The name of the evaluated feature.
            - metrics_avg (dict): A dictionary containing the average metrics
                                  (precision, recall, f1, accuracy, auroc)
                                  across all seeds and folds.
            - all_metrics (np.array): A 2D array storing individual metric scores
                                      for each (seed * fold) iteration.
            - all_real (np.array): Concatenated array of true labels from all test folds.
            - all_pred_prob (np.array): Concatenated array of predicted probabilities
                                        for the positive class from all test folds.
    """

    # Reshape X to be a 2D array (required by scikit-learn for single features) and flatten y to ensure it's a 1D array.
    X = X.to_numpy().reshape(-1, 1)
    y = y.to_numpy().flatten()

    # Define the names of the metrics that will be collected
    metrics_names = [
        'precision_0', 'recall_0', 'f1_0',  # Metrics for class 0
        'precision_1', 'recall_1', 'f1_1',  # Metrics for class 1
        'accuracy', 'auroc'                 # Overall accuracy and AUC-ROC
    ]

    # Initialize a NumPy array to store all collected metrics across all iterations
    # The size is (num_seeds * num_folds) rows by (number of metrics) columns
    all_metrics = np.zeros((num_seeds * num_folds, len(metrics_names)))
    
    # Initialize lists to store true labels and predicted probabilities from test sets
    # These will be concatenated at the end.
    all_real = []
    all_pred_prob = []

    idx = 0

    # Loop through each random seed
    for seed in range(num_seeds):
        # Initialize StratifiedKFold for cross-validation.
        # `shuffle=True` shuffles the data before splitting.
        # `random_state=seed` ensures the splits are reproducible for the current seed.
        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)

        # Loop through each train/test split generated by StratifiedKFold for the current seed
        for train_idx, test_idx in skf.split(X, y):
            # Split the data into training and testing sets for the current fold
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # Initialize a Decision Tree Classifier with max_depth=1 (a decision stump).
            # `class_weight='balanced'` adjusts weights inversely proportional to class frequencies,
            # which is useful for imbalanced datasets.
            # `random_state=seed` ensures reproducibility of the tree's internal randomness.
            clf = DecisionTreeClassifier(max_depth=1, class_weight='balanced', random_state=seed)
            clf.fit(X_train, y_train) # Train the classifier on the training data

            # Get predicted labels and predicted probabilities for the test set
            y_test_pred = clf.predict(X_test)
            y_test_prob = clf.predict_proba(X_test)[:, 1] # Probability of the positive class (class 1)

            # Generate a classification report for the test set (as a dictionary)
            # `zero_division=0` handles cases where a class has no samples in a fold, preventing warnings.
            report = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)
            
            # Calculate AUC-ROC score
            auroc = roc_auc_score(y_test, y_test_prob)
            
            # Extract overall accuracy from the classification report
            accuracy = report['accuracy']

            # Create a list (row) of all metrics for the current fold
            row = [
                report['0']['precision'], report['0']['recall'], report['0']['f1-score'],
                report['1']['precision'], report['1']['recall'], report['1']['f1-score'],
                accuracy, auroc
            ]

            # Store the row of metrics in the `all_metrics` array
            all_metrics[idx, :] = row
            
            # Append true labels and predicted probabilities to their respective lists
            all_real.append(y_test)
            all_pred_prob.append(y_test_prob)

            idx += 1 # Increment the index for the next row in all_metrics

    # Concatenate all true labels and predicted probabilities from all folds and seeds
    all_real = np.concatenate(all_real)
    all_pred_prob = np.concatenate(all_pred_prob)
    
    # Calculate the average of each metric across all iterations (seeds * folds)
    metrics_avg = {name: float(np.mean(all_metrics[:, i])) for i, name in enumerate(metrics_names)}

    # Return the results
    return feature_name, metrics_avg, all_metrics, all_real, all_pred_prob


# --- Main code for running evaluations and exporting results ---

REPETITIONS = 100 # Number of times to repeat the k-fold cross-validation with different seeds
N_FOLDS = 5       # Number of folds for k-fold cross-validation
N_JOBS = 6        # Number of CPU cores to use for parallel processing (set to -1 for all available cores)

# Define the filename for the dataset. Ensure this path is correct for your environment.
filename = 'dataset.csv' 
df = pd.read_csv(filename) # Load the dataset into a pandas DataFrame
df = df[df["group"] == "double_5fu"] # Filter the DataFrame to include only rows where "group" is "double_5fu"

df_new = dict() # Initialize an empty dictionary to store newly engineered features

# --- Feature Engineering Loop ---
# This nested loop systematically generates new features by creating linear combinations
# of existing biomarker columns ('cd8', 'cd4', 'cd11b_gr1') using various weights.
for w_cd8 in np.arange(-6, 6, 0.5): # Iterate over weights for 'cd8' from -6 to 6 with step 0.5
    if w_cd8 != 0: # Exclude cases where the weight for 'cd8' is zero
        label_cd8 = f'{w_cd8:+}cd8' # Create a label for the cd8 component (e.g., "+1.0cd8")
        for w_cd4 in np.arange(-6, 6, 0.5): # Iterate over weights for 'cd4'
            if w_cd4 != 0: # Exclude cases where the weight for 'cd4' is zero
                label_cd4 = f'{w_cd4:+}cd4' # Create a label for the cd4 component
                for w_cd11b in np.arange(-6, 6, 0.5): # Iterate over weights for 'cd11b_gr1'
                    if w_cd11b != 0: # Exclude cases where the weight for 'cd11b_gr1' is zero
                        label_cd11b = f'{w_cd11b:+}cd11b_gr1' # Create a label for the cd11b_gr1 component
                        
                        # Construct a unique column name for the new engineered feature
                        col_name = f'{label_cd8}{label_cd4}{label_cd11b}'
                        
                        # Calculate the new feature as a linear combination of existing ones
                        df_new[col_name] = (w_cd8 * df['cd8'] +
                                            w_cd4 * df['cd4'] +
                                            w_cd11b * df['cd11b_gr1'])

df_new = pd.DataFrame(df_new) # Convert the dictionary of new features into a DataFrame
y = df['survival_day_25']     # Extract the target variable 'survival_day_25' from the original DataFrame

# --- Parallel Processing of Feature Evaluation ---
# Use joblib.Parallel to run the evaluator_kfold function for each engineered feature
# in parallel, significantly speeding up the computation.
# `tqdm` wraps the feature names iteration to display a progress bar in the console/notebook.
results = Parallel(n_jobs=N_JOBS, prefer="processes")( # `prefer="processes"` uses multiprocessing
    delayed(evaluator_kfold)(df_new[feature_name], y, feature_name, REPETITIONS, N_FOLDS)
    for feature_name in tqdm(df_new.columns, desc="Processing feature combinations") # Progress bar description
)

features = []      # List to store feature names
metrics_list = []  # List to store average metrics for each feature

# Process the results from parallel execution
for res in results:
    # Unpack the results tuple: feature_name, average_metrics_dict, _, _, _
    # The asterisks `*_` are used to ignore the `all_metrics`, `all_real`, and `all_pred_prob`
    # arrays returned by `evaluator_kfold` as they are not needed in this part.
    feature_name, metrics_avg, *_ = res
    features.append(feature_name)   # Add the feature name to the list
    metrics_list.append(metrics_avg) # Add the dictionary of average metrics

df_metrics = pd.DataFrame(metrics_list, index=features) # Create a DataFrame from the collected metrics,
                                                        # using feature names as the index.

# Rename columns for better readability and consistency
df_metrics.rename(columns={
    'precision_0': 'Precision_Class_0',
    'recall_0': 'Recall_Class_0',
    'f1_0': 'F1_Class_0',
    'precision_1': 'Precision_Class_1',
    'recall_1': 'Recall_Class_1',
    'f1_1': 'F1_Class_1',
    'accuracy': 'Accuracy',
    'auroc': 'AUC_ROC'
}, inplace=True) # Apply changes directly to the DataFrame

# Export the DataFrame containing all averaged metrics to a CSV file
df_metrics.to_csv('metrics_results.csv', index_label='Feature') # 'Feature' will be the header for the index column